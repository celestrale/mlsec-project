{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Crafting ensemble adversarial attacks & testing their transferability\n",
    "### Machine Learning Security Project 2 - Jules COOPER, Leonie MAIER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Project instructions:**\n",
    "Consider 3 models from RobustBench (CIFAR10, L-inf) and craft universal (and untargeted) adversarial examples aimed to fool the 3 models at the same time. Evaluate transferability of such adversarial examples to other 7 models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import secml\n",
    "    import foolbox\n",
    "except ImportError:\n",
    "    %pip install git+https://github.com/pralab/secml\n",
    "    %pip install foolbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_attack(attack_array, name):\n",
    "    with open(name+\".txt\", 'w') as f:\n",
    "        for line in attack_array:\n",
    "            f.write(\"\".join(str(line)) + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The models from RobustBench (Linf, CIFAR-10) tested were:\n",
    "\n",
    "| Model name | Model ID | Clean Accuracy  | Robust Accuracy  |  Architecture  |\n",
    "|----|----|----|----|---|\n",
    "| secml_model1 | Ding2020MMA | 84.36% | 41.44% | WideResNet-28-4 |\n",
    "| secml_model2 | Wong2020Fast | 83.34% | 43.21% | PreActResNet-18 |\n",
    "| secml_model3 | Andriushchenko2020Understanding | 79.84% | 43.93% | PreActResNet-18 |\n",
    "| secml_model4 | Bartoldson2024Adversarial_WRN-94-16 | 93.68% | 73.71% | WideResNet-94-16 |\n",
    "| secml_model5 | Sehwag2021Proxy_ResNest152 | 87.30% | 62.79% | ResNest152 |\n",
    "| secml_model6 | Huang2021Exploring | 90.56% | 61.56% | WideResNet-34-R |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from secml.data.loader.c_dataloader_cifar import CDataLoaderCIFAR10\n",
    "from secml.ml.classifiers import CClassifierPyTorch\n",
    "from secml.ml.features.normalization import CNormalizerMinMax\n",
    "\n",
    "\n",
    "train_ds, test_ds = CDataLoaderCIFAR10().load()\n",
    "dataset_labels = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "normalizer = CNormalizerMinMax().fit(train_ds.X)\n",
    "\n",
    "from robustbench.utils import load_model\n",
    "\n",
    "model1 = load_model(model_name='Ding2020MMA', dataset='cifar10', threat_model='Linf')\n",
    "secml_model1 = CClassifierPyTorch(model1, input_shape=(3,32,32), pretrained=True)\n",
    "model2 = load_model(model_name='Wong2020Fast', dataset='cifar10', threat_model='Linf')\n",
    "secml_model2 = CClassifierPyTorch(model2, input_shape=(3,32,32), pretrained=True)\n",
    "model3 = load_model(model_name='Andriushchenko2020Understanding', dataset='cifar10', threat_model='Linf')\n",
    "secml_model3 = CClassifierPyTorch(model3, input_shape=(3,32,32), pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model4 = load_model(model_name='Bartoldson2024Adversarial_WRN-94-16', dataset='cifar10', threat_model='Linf')\n",
    "secml_model4 = CClassifierPyTorch(model4, input_shape=(3,32,32), pretrained=True)\n",
    "\n",
    "model5 = load_model(model_name='Sehwag2021Proxy_ResNest152', dataset='cifar10', threat_model='Linf')\n",
    "secml_model5 = CClassifierPyTorch(model5, input_shape=(3,32,32), pretrained=True)\n",
    "\n",
    "model6 = load_model(model_name='Huang2021Exploring', dataset='cifar10', threat_model='Linf')\n",
    "secml_model6 = CClassifierPyTorch(model6, input_shape=(3,32,32), pretrained=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from secml.ml.classifiers.loss import CLossCrossEntropy\n",
    "from secml.array import CArray\n",
    "\n",
    "\n",
    "def pgd_linf_mult_untargeted(x, y, models, eps, alpha, steps):\n",
    "    \"\"\"Performs a Projected Gradient Descent attack on one or more models by averaging their gradient at\"\"\"\n",
    "\n",
    "    # we need the gradient of the softmax\n",
    "    for clf in models :\n",
    "            clf.softmax_outputs = True\n",
    "    \n",
    "    # Using cross entropy as loss function\n",
    "    loss_func = CLossCrossEntropy()\n",
    "    x_adv = x.deepcopy()  \n",
    "\n",
    "    for i in range(steps):\n",
    "        gradients = []\n",
    "        # Follow progression of attack\n",
    "        if i % 10 == 0 : \n",
    "                print((i/steps)*100,\"%\")\n",
    "\n",
    "        # Calculate scores and gradients of each model        \n",
    "        for clf_index in range(len(models)):\n",
    "                                       \n",
    "            scores = models[clf_index].decision_function(x_adv)\n",
    "\n",
    "            # gradient of the loss considering the output logits\n",
    "            loss_gradient = loss_func.dloss(y_true=y, score=scores)\n",
    "            # gradient of the output logits considering the input\n",
    "            clf_gradient = models[clf_index].grad_f_x(x_adv, y)\n",
    "\n",
    "            # gradient of the loss function considering the input\n",
    "            gradient = clf_gradient * loss_gradient\n",
    "\n",
    "            gradients.append(gradient)\n",
    "\n",
    "\n",
    "        gradient = gradients[0]\n",
    "\n",
    "        # If multiple models, calculate the mean gradient\n",
    "        if (len(gradients) > 1):\n",
    "            gradient = CArray([arr.tondarray() for arr in gradients])\n",
    "            gradient = gradient.mean(axis=0)\n",
    "\n",
    "        # Normalize mean gradient\n",
    "        gradient = gradient.sign()\n",
    "\n",
    "        # make step\n",
    "        x_adv = x_adv + alpha * gradient\n",
    "\n",
    "        # project inside epsilon-ball : For Linf, only need to keep it between epsilon boundaries\n",
    "        delta = (x_adv - x).clip(-eps,eps)\n",
    "        x_adv = x + delta\n",
    "\n",
    "        # force input bounds\n",
    "        x_adv = x_adv.clip(0, 1)\n",
    "\n",
    "    predict = []\n",
    "    \n",
    "\n",
    "    for clf in models:\n",
    "        #Restore outputs\n",
    "        clf.softmax_outputs = False\n",
    "        # Add prediction of each classifier\n",
    "        predict.append(clf.predict(x_adv))\n",
    "\n",
    "    return x_adv, predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def multiple_runs(iterations, models, eps, alpha, steps):\n",
    "    \"\"\"Allows to run the PGD attack multiple times\"\"\"\n",
    "\n",
    "    success = 0\n",
    "    for n in range(iterations):\n",
    "        # Choosing a random image from the test set\n",
    "        i = random.randint(0,10000)\n",
    "        pt = test_ds[i, :]\n",
    "        x0, y0 = pt.X, pt.Y\n",
    "\n",
    "        # Normalizing the input\n",
    "        x0 = normalizer.transform(x0)\n",
    "\n",
    "        print(f\"attack_number {n} on image {i}\")\n",
    "        print(f\"Starting point has label: {dataset_labels[y0.item()]}\")\n",
    "\n",
    "        # Run one (combined) attack \n",
    "        x_adv, y_advs = pgd_linf_mult_untargeted(x0, y0, models, eps, alpha, steps)\n",
    "        \n",
    "        adversarial = True\n",
    "        for y_adv in y_advs:\n",
    "            print(f\"Adversarial point has label: {dataset_labels[y_adv.item()]}\")\n",
    "            # If at least one model still predicts real label after attack, unsuccessful attack\n",
    "            if y_adv.item() == y0.item():\n",
    "                adversarial = False\n",
    "        \n",
    "        if adversarial:\n",
    "            # Successful attack : Update statistics and save input values of adversarial example\n",
    "            success +=1\n",
    "            save_attack(x_adv, f\"attack_image_{i}\")\n",
    "\n",
    "    print(f\"statistic over {n} attacks: {(success/n)}\")\n",
    "    print(f\"sucess: {success}\")\n",
    "    print(f\"failure: {iterations - success}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# High number of steps to try for best accuracy\n",
    "steps = 100\n",
    "# Standard Linf budget\n",
    "eps = 8/255\n",
    "alpha = eps/2\n",
    "\n",
    "# Running on less robust models\n",
    "multiple_runs(100,[secml_model1, secml_model2, secml_model3],eps,alpha,steps)\n",
    "\n",
    "# Running on more robust models\n",
    "multiple_runs(100,[secml_model4, secml_model5, secml_model6],eps,alpha,steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results\n",
    "\n",
    "Results on two ensembles of models for 100 iterations:\n",
    "\n",
    "| Model names | Model IDs | Attack success |\n",
    "|----|----|----|\n",
    "| secml_model1, secml_model2, secml_model3 | Ding2020MMA, Wong2020Fast, Andriushchenko2020Understanding | 39% |\n",
    "| secml_model4, secml_model5, secml_model6 | Bartoldson2024Adversarial_WRN-94-16, Sehwag2021Proxy_ResNest152, Huang2021Exploring | 22 % |\n",
    "\n",
    "The attack success rate does not exactly inversely correlate with the adversarial robustness of the individual models. This can be explained by the fact that since the strategy used to create a universal adversarial example is to calculate the mean of gradients between the models, it's possible that some cancel each other out and thus do not allow to achieve significant steps towards a different class. Also, some partial successes (where the resulting example fools some of the models but not all), were not counted as successful attacks. This also follows the tendency of ensembles being more robust than individual models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transferability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The models from RobustBench (Linf, CIFAR-10) tested were:\n",
    "\n",
    "| Model ID | Clean Accuracy  | Robust Accuracy  |  Architecture  |\n",
    "|----|----|----|---|\n",
    "| Amini2024MeanSparse_Ra_WRN_70_16 | 93.24% | 68.94% | MeanSparse RaWideResNet-70-16 |\n",
    "| Gowal2021Improving_70_16_ddpm_100m | 88.74% | 66.10% | WideResNet-70-16 |\n",
    "| Cui2023Decoupled_WRN-28-10 | 92.16% | 67.73% | WideResNet-28-10 |\n",
    "| Wang2023Better_WRN-28-10 | 92.44% | 67.31% | WideResNet-28-10 |\n",
    "| Rebuffi2021Fixing_106_16_cutmix_ddpm | 88.50% | 64.58% | WideResNet-106-16 |\n",
    "| Huang2022Revisiting_WRN-A4 | 91.58% | 62.79% | WideResNet-A4 |\n",
    "| Kang2021Stable | 93.73% | 64.20% | WideResNet-70-16, Neural ODE block |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading every models to the variable test_models\n",
    "transfer_models_name = [\"Amini2024MeanSparse_Ra_WRN_70_16\", \"Gowal2021Improving_70_16_ddpm_100m\", \"Cui2023Decoupled_WRN-28-10\", \"Wang2023Better_WRN-28-10\", \"Rebuffi2021Fixing_106_16_cutmix_ddpm\", \"Huang2022Revisiting_WRN-A4\", \"Kang2021Stable\"]\n",
    "test_models = []\n",
    "for model_name_t in transfer_models_name:\n",
    "\n",
    "    model = load_model(model_name=model_name_t, dataset='cifar10', threat_model='Linf')\n",
    "    secml_model = CClassifierPyTorch(model, input_shape=(3,32,32), pretrained=True)\n",
    "    test_models.append(secml_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import re\n",
    "\n",
    "\n",
    "def transferability_attacks(models, path):\n",
    "    number_success = 0\n",
    "    total_run = 0\n",
    "    success_models=[]\n",
    "    total_local_models=[]\n",
    "\n",
    "\n",
    "    for i in range(len(models)):\n",
    "        success_models.append(0)\n",
    "        total_local_models.append(0)\n",
    "\n",
    "\n",
    "    for f in listdir(path):\n",
    "        if not isfile(join(path, f)):\n",
    "            continue\n",
    "        if \"attack_image\" not in f:\n",
    "            continue\n",
    "\n",
    "        # extract the id of the picture associated with the attacks to get the associated class\n",
    "        nums = re.findall(r'\\d+', f)\n",
    "        pt = test_ds[int(nums[0]), :]\n",
    "        x0, y0 = pt.X, pt.Y\n",
    "\n",
    "        # Load adversarial example values into array\n",
    "        arr = []\n",
    "        with open(join(path, f), 'r') as f:\n",
    "            for l in f.readlines():\n",
    "                arr.append(float(l))\n",
    "        x_adv = CArray([arr])\n",
    "        \n",
    "        # run the prediction with the adversarial example\n",
    "        for i in range(len(models)):\n",
    "            \n",
    "            y_pred = models[i].predict(x_adv)\n",
    "            if (y_pred.item() != y0.item()):\n",
    "                number_success+=1\n",
    "                success_models[i]+=1\n",
    "\n",
    "            total_local_models[i]+=1\n",
    "            total_run +=1\n",
    "\n",
    "    print(f\"{total_local_models[0]} attacks were transfered other {len(models)} models\")\n",
    "    print(f\"Within the {total_run} attacks, {number_success} succeeded: accuracy {number_success/total_run*100}%\")\n",
    "    print(\"Individual model statistics\")\n",
    "    \n",
    "\n",
    "    for i in range(len(models)):\n",
    "        print(f\"\\tModel {i} had {success_models[i]} attacks succeed, transferability: {success_models[i]/total_local_models[i]*100}% over the {total_local_models[i]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transferability_attacks(test_models, \"attacks/model1-2-3\")\n",
    "\n",
    "transferability_attacks(test_models, \"attacks/model4-5-6\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Result\n",
    "\n",
    "The models from RobustBench for analysing transferability were tested on the different adversarial examples previously found.\n",
    "\n",
    "**Adversarial example found with models 1, 2 and 3**\n",
    "\n",
    "| Model ID | Successful Transferability |  Architecture  |\n",
    "|----|----|---|\n",
    "| Amini2024MeanSparse_Ra_WRN_70_16 | 51.28% | MeanSparse RaWideResNet-70-16 |\n",
    "| Gowal2021Improving_70_16_ddpm_100m | 58.97% | WideResNet-70-16 |\n",
    "| Cui2023Decoupled_WRN-28-10 | 58.97% | WideResNet-28-10 |\n",
    "| Wang2023Better_WRN-28-10 | 53.84% | WideResNet-28-10 |\n",
    "| Rebuffi2021Fixing_106_16_cutmix_ddpm | 61.53% | WideResNet-106-16 |\n",
    "| Huang2022Revisiting_WRN-A4 | 46.15% | WideResNet-A4 |\n",
    "| Kang2021Stable | 38.46% | WideResNet-70-16, Neural ODE block |\n",
    "\n",
    "Overall transferability of 52.74%\n",
    "\n",
    "\n",
    "**Adversarial example found with models 4, 5 and 6**\n",
    "\n",
    "| Model ID | Successful Transferability |  Architecture  |\n",
    "|----|----|---|\n",
    "| Amini2024MeanSparse_Ra_WRN_70_16 | 86.36% | MeanSparse RaWideResNet-70-16 |\n",
    "| Gowal2021Improving_70_16_ddpm_100m | 95.45% | WideResNet-70-16 |\n",
    "| Cui2023Decoupled_WRN-28-10 | 100% | WideResNet-28-10 |\n",
    "| Wang2023Better_WRN-28-10 | 95.45% | WideResNet-28-10 |\n",
    "| Rebuffi2021Fixing_106_16_cutmix_ddpm | 95.45% | WideResNet-106-16 |\n",
    "| Huang2022Revisiting_WRN-A4 | 86.36% | WideResNet-A4 |\n",
    "| Kang2021Stable | 68.18% | WideResNet-70-16, Neural ODE block |\n",
    "\n",
    "Overall transferability of 89.61%\n",
    "\n",
    "\n",
    "**Comments**\n",
    "\n",
    "The transferability seems to be higher using the adversarial examples that were found by the models 4,5 and 6. This could be explained by the fact that since these models are more robust against adversarial examples, the ones that do lead to a successful missclassification have made modifications that are more significant and detailed, rather than a more generic modification that was sufficient to fool the less robust ensemble of models 1,2 and 3.\n",
    "Also most of the models used for transferability are more robust than the models 1, 2 and 3, meaning that an easy attack on the weaker ensemble cannot transfer as easily."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
